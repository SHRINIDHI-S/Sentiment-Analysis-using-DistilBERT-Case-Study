# -*- coding: utf-8 -*-
"""Case Study 1 .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qQrJxiBuWPGEcdM65e6JXROdB9NELXdA

# ðŸš€ Sentiment Analysis using DistilBERT on CPU

## ðŸ“Œ Overview
This project builds a **lightweight Sentiment Analysis Model** using **DistilBERT** that:  
âœ… **Classifies** text into **Positive** or **Negative**  
âœ… **Trains quickly with minimal resources**  
âœ… **Deploys as an API using FastAPI**

## ðŸ› ï¸ Project Steps
1ï¸âƒ£ **Install Dependencies**  
2ï¸âƒ£ **Load a Small Sentiment Analysis Dataset**  
3ï¸âƒ£ **Preprocess & Tokenize Text**  
4ï¸âƒ£ **Train a DistilBERT Model on CPU**  
5ï¸âƒ£ **Deploy as an API using FastAPI**

## ðŸ“Œ Step 1: Install Dependencies
"""

#pip install transformers datasets torch fastapi uvicorn

"""## ðŸ“Œ Step 2: Load a Small Sentiment Analysis Dataset

We need a dataset to train the model. **Yelp Review dataset** contains movie reviews labeled as **positive (1)** or **negative (0)**.

#### Importing Required Libraries
The script begins by importing the necessary libraries:

- `torch`: Provides deep learning functionalities.
- `pandas`: Used for handling and processing structured data.
- `datasets`: Loads NLP datasets from Hugging Face.
- `transformers`: Contains pretrained NLP models and training utilities.
- `sklearn.model_selection`: Used to split the dataset into training and test sets.

#### Loading the Yelp Review Dataset
The dataset is loaded using the `datasets` library. It selects **2000 samples** from the Yelp review dataset's training split and converts it into a Pandas DataFrame for easy manipulation.

#### Converting Star Ratings to Binary Sentiment Labels
The dataset originally contains **star ratings from 1 to 5**:
- **3-star reviews** are removed to eliminate neutral sentiments.
- Ratings **4 and 5** are labeled as **positive (1)**.
- Ratings **1 and 2** are labeled as **negative (0)**.
This conversion allows the dataset to be used for binary sentiment classification.

#### Shuffling the Dataset
To ensure randomness, the dataset is shuffled using `df.sample(frac=1, random_state=42)`. The `random_state` parameter ensures that the shuffle order remains consistent across different runs.

#### Displaying Dataset Information
To validate the preprocessing steps:
- `df.head()` prints a preview of the first few records.
- `df["label"].value_counts()` checks the distribution of positive and negative samples to verify balance in the dataset.

This processed dataset is now ready to be used for training a **binary sentiment classification model**.
"""

import torch
import pandas as pd
from datasets import load_dataset, Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split

from datasets import load_dataset
import pandas as pd

# Load the IMDb dataset (only 2000 samples for quick training)
dataset = load_dataset("yelp_review_full", split="train[:2000]")
df = pd.DataFrame(dataset)


# Convert star ratings to binary sentiment labels
df = df[df["label"] != 2]  # Remove 3-star neutral reviews
df["label"] = df["label"].apply(lambda x: 1 if x > 2 else 0)  # 4-5 stars -> Positive (1), 1-2 stars -> Negative (0)

# Shuffle dataset
df = df.sample(frac=1, random_state=42)

# Display dataset info
print(df.head())
print(df["label"].value_counts())  # Check label distribution

"""## ðŸ“Œ Step 3: Data Preprocessing

- Raw text might contain **extra spaces or unnecessary symbols**.  
- **Clean text** to improve training efficiency.

"""

# ðŸ“Œ Clean and Preprocess Text
import re

def clean_text(text):
    """Remove extra spaces and unwanted characters."""
    text = text.strip()
    text = re.sub(r'\s+', ' ', text)
    return text

df["text"] = df["text"].apply(clean_text)
print("âœ… Text cleaning completed!")

"""## ðŸ“Œ Step 4: Tokenization

- ML models cannot **directly** process raw text  
- Convert text into **numerical representations (tokens)**  
- **DistilBERT tokenizer** converts words into **token IDs**  

"""

from transformers import AutoTokenizer

# ðŸ“Œ Tokenization
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def tokenize_function(example):
    """Tokenize text with DistilBERT tokenizer."""
    encoding = tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=256,
    )
    encoding["label"] = example["label"]  # Add labels
    return encoding

# Apply tokenization
df["tokens"] = df.apply(tokenize_function, axis=1)
print("âœ… Tokenization completed!")

"""## ðŸ“Œ Step 5: Load DistilBERT Model & Train on CPU

- We **use DistilBERT** (lighter version of BERT)  
- Fine-tune the model for **sentiment analysis**    

"""

# ðŸ“Œ Prepare Data for Training
# Convert tokenized text into input IDs and attention masks
df["input_ids"] = df["tokens"].apply(lambda x: x["input_ids"])
df["attention_mask"] = df["tokens"].apply(lambda x: x["attention_mask"])

# Drop unnecessary columns
df = df.drop(columns=["tokens"])

# Train-test split (80% train, 20% validation)
train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_pandas(train_df)
eval_dataset = Dataset.from_pandas(eval_df)

print(f"âœ… Train size: {len(train_dataset)}, Eval size: {len(eval_dataset)}")

import torch
from transformers import AutoModelForSequenceClassification

# âœ… Load DistilBERT model for Sentiment Analysis (Binary Classification: Positive/Negative)
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)

# âœ… Explicitly use CPU
device = torch.device("cpu")  # Ensures no GPU usage
model.to(device)

print("âœ… Model loaded successfully on CPU!")

# ðŸ“Œ Automatically Select GPU if Available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"âœ… Using Device: {device}")

# ðŸ“Œ Load DistilBERT Model
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)
model.to(device)

print("âœ… Model loaded successfully!")

"""## ðŸ“Œ **Training the DistilBERT Sentiment Model**
### ðŸ”§ **Key Steps**
âœ” Load the **DistilBERT model** with `num_labels=2` (Binary classification)  
âœ” Define **training parameters** optimized for CPU  
âœ” Use the **Hugging Face Trainer API** to train the model  

"""

# âœ… Define Training Arguments
training_args = TrainingArguments(
    output_dir="./distilbert_sentiment_cpu",  # Save the model output
    per_device_train_batch_size=4,  # Small batch size for CPU
    num_train_epochs=2,  # Keep it small for quick training
    save_strategy="epoch",
    evaluation_strategy="epoch",
    logging_dir="./logs",
    learning_rate=3e-5,
    save_total_limit=2,
    no_cuda=True,  # âœ… Force CPU usage
)

# âœ… Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,  # Include evaluation dataset to avoid errors
)

# âœ… Start Training
trainer.train()

print("âœ… Model Training Complete!")

"""## ðŸ“Œ **Evaluating Model Performance**
### ðŸ”§ **Key Steps**
âœ” Compute **accuracy** on the evaluation dataset  
âœ” Use `Trainer.evaluate()` to get loss & metrics  

"""

# âœ… Evaluate the model
eval_results = trainer.evaluate()
print(f"âœ… Evaluation Results: {eval_results}")

"""## ðŸ“Œ **Saving the Model for Deployment**
### ðŸ”§ **Key Steps**
âœ” Save model & tokenizer for later inference  
âœ” Use `save_pretrained()` for easy loading  

"""

# âœ… Save trained model
model.save_pretrained("./distilbert_sentiment_model")
tokenizer.save_pretrained("./distilbert_sentiment_model")

print("âœ… Model and Tokenizer Saved Successfully!")

"""## ðŸ“Œ **Testing the Sentiment Analysis Model**
### ðŸ”§ **Key Steps**
âœ” Tokenize a **new sentence**  
âœ” Use model **to predict sentiment (positive/negative)**  
âœ” Convert **logits to probabilities** using softmax  

"""

import torch
from transformers import AutoTokenizer

# âœ… Load the trained model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("./distilbert_sentiment_model")
model = AutoModelForSequenceClassification.from_pretrained("./distilbert_sentiment_model")
model.to("cpu")  # âœ… Force CPU usage

# âœ… Function to predict sentiment
def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=256)
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    predicted_class = torch.argmax(logits, dim=1).item()

    sentiment = "Positive" if predicted_class == 1 else "Negative"
    return sentiment

# âœ… Test the model with new input
#test_sentence = "I absolutely hated the movie! It was bad."
test = input("Enter a string:")
#predicted_sentiment = predict_sentiment(test_sentence)
predicted_sentiment = predict_sentiment(test)


print(f"âœ… Sentiment Prediction: {predicted_sentiment}")

"""## ðŸ“Œ **Deploying Sentiment Model as an API**
### ðŸ”§ **Key Steps**
âœ” Use **FastAPI** to create an API endpoint  
âœ” Accept **user input** & return **sentiment predictions**  
âœ” Run API with **Uvicorn**  

"""